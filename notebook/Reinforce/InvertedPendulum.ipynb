{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khởi tạo môi trường\n",
    "\n",
    "env_name = \"InvertedPendulum-v4\"\n",
    "# Create and wrap the environment\n",
    "env = gym.make(env_name)\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Records episode-reward\n",
    "\n",
    "# Observation-space of InvertedPendulum-v4 (4)\n",
    "obs_space_dims = env.observation_space.shape[0]\n",
    "# Action-space of InvertedPendulum-v4 (1)\n",
    "action_space_dims = env.action_space.shape[0]\n",
    "rewards_over_seeds = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo folder kết quả\n",
    "\n",
    "import os\n",
    "def mkdir(path): \n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "save_path = \"./save\"\n",
    "env_path = f'{save_path}/{env_name}'\n",
    "model_path = f'{save_path}/{env_name}/model'\n",
    "demo_path = f'{save_path}/{env_name}/demo'\n",
    "\n",
    "mkdir(save_path)\n",
    "mkdir(env_path)\n",
    "mkdir(model_path)\n",
    "mkdir(demo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khai báo thuật toán + train model\n",
    "\n",
    "from algorithm.Reinforce import REINFORCE\n",
    "\n",
    "seed_set = [1]\n",
    "total_num_episodes = 1  # Total number of episodes\n",
    "for seed in seed_set:  # Fibonacci seeds\n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Reinitialize agent every seed\n",
    "    agent = REINFORCE(obs_space_dims, action_space_dims, device = device)\n",
    "    reward_over_episodes = []\n",
    "\n",
    "    for episode in range(total_num_episodes):\n",
    "        # gymnasium v26 requires users to set seed while resetting the environment\n",
    "        obs, info = wrapped_env.reset(seed=seed)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.sample_action(obs)\n",
    "\n",
    "            # Step return type - `tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]`\n",
    "            # These represent the next observation, the reward from the step,\n",
    "            # if the episode is terminated, if the episode is truncated and\n",
    "            # additional info from the step\n",
    "            obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "            agent.rewards.append(reward)\n",
    "\n",
    "            # End the episode when either truncated or terminated is true\n",
    "            #  - truncated: The episode duration reaches max number of timesteps\n",
    "            #  - terminated: Any of the state space values is no longer finite.\n",
    "            done = terminated or truncated\n",
    "\n",
    "        reward_over_episodes.append(wrapped_env.return_queue[-1])\n",
    "        agent.update()\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            avg_reward = int(np.mean(wrapped_env.return_queue))\n",
    "            print(\"Episode:\", episode, \"Average Reward:\", avg_reward)\n",
    "\n",
    "    rewards_over_seeds.append(reward_over_episodes)\n",
    "\n",
    "    save_model = f'{model_path}/{env_name}_reinforce_seed_{seed}.pth'\n",
    "    torch.save(agent, save_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot kết quả\n",
    "\n",
    "rewards_to_plot = [[reward[0] for reward in rewards] for rewards in rewards_over_seeds]\n",
    "df1 = pd.DataFrame(rewards_to_plot).melt()\n",
    "df1.rename(columns={\"variable\": \"episodes\", \"value\": \"reward\"}, inplace=True)\n",
    "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\")\n",
    "sns.lineplot(x=\"episodes\", y=\"reward\", data=df1).set(\n",
    "    title=\"REINFORCE for InvertedPendulum-v4\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# Function to visualize the trained model\n",
    "def visualize_trained_model(agent, env_name=\"InvertedPendulum-v4\", num_episodes=1, seed=1):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    frames = []\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset(seed=seed)\n",
    "        done = False\n",
    "        while not done:\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            action = agent.sample_action(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    # Create animation\n",
    "    fig = plt.figure()\n",
    "    plt.axis('off')\n",
    "    im = plt.imshow(frames[0])\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_array(frame)\n",
    "        return [im]\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=frames, interval=50)\n",
    "    plt.close()\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "    frames = [Image.fromarray(frame) for frame in frames]\n",
    "    frames[0].save(f'{demo_path}\\{env_name}_reinforce_seed_{seed}.gif', save_all=True, append_images=frames[1:], loop=0, duration=50)\n",
    "    \n",
    "    return ani\n",
    "\n",
    "# Assuming the REINFORCE class and agent are defined and trained as in your provided code\n",
    "visualize_trained_model(agent, env_name=\"InvertedPendulum-v4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tue.cm210908",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
