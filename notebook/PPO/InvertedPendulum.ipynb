{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#import libs\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khởi tạo môi trường\n",
    "\n",
    "env_name = \"InvertedPendulum-v4\"\n",
    "# Create and wrap the environment\n",
    "env = gym.make(env_name)\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Records episode-reward\n",
    "\n",
    "# Observation-space of InvertedPendulum-v4 (4)\n",
    "obs_space_dims = env.observation_space.shape[0]\n",
    "# Action-space of InvertedPendulum-v4 (1)\n",
    "action_space_dims = env.action_space.shape[0]\n",
    "rewards_over_seeds = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo folder kết quả\n",
    "\n",
    "import os\n",
    "def mkdir(path): \n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "save_path = \"./save\"\n",
    "env_path = f'{save_path}/{env_name}'\n",
    "model_path = f'{save_path}/{env_name}/model'\n",
    "demo_path = f'{save_path}/{env_name}/demo'\n",
    "\n",
    "mkdir(save_path)\n",
    "mkdir(env_path)\n",
    "mkdir(model_path)\n",
    "mkdir(demo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khai báo thuật toán\n",
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=torch.nn.Tanh,\n",
    "    net_arch=[dict(pi=[128, 128], vf=[128, 128])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize the PPO agent\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, seed=seed, learning_rate=learning_rate)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# model = TRPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, seed=seed)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, total_num_episodes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, evaluation_interval):\n",
      "File \u001b[0;32m~/miniconda3/envs/tue.cm210908/lib/python3.11/site-packages/stable_baselines3/sac/sac.py:157\u001b[0m, in \u001b[0;36mSAC.__init__\u001b[0;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, ent_coef, target_update_interval, target_entropy, use_sde, sde_sample_freq, use_sde_at_warmup, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ment_coef_optimizer: Optional[th\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tue.cm210908/lib/python3.11/site-packages/stable_baselines3/sac/sac.py:160\u001b[0m, in \u001b[0;36mSAC._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_aliases()\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# Running mean and running var\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tue.cm210908/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:199\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer_class(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size,\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreplay_buffer_kwargs,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Convert train freq parameter to TrainFreq object\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tue.cm210908/lib/python3.11/site-packages/stable_baselines3/sac/policies.py:278\u001b[0m, in \u001b[0;36mSACPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, use_sde, log_std_init, use_expln, clip_mean, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs, n_critics, share_features_extractor)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    269\u001b[0m     {\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_critics\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_critics,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m     }\n\u001b[1;32m    274\u001b[0m )\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor \u001b[38;5;241m=\u001b[39m share_features_extractor\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tue.cm210908/lib/python3.11/site-packages/stable_baselines3/sac/policies.py:281\u001b[0m, in \u001b[0;36mSACPolicy._build\u001b[0;34m(self, lr_schedule)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build\u001b[39m(\u001b[38;5;28mself\u001b[39m, lr_schedule: Schedule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_class(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m    284\u001b[0m         lr\u001b[38;5;241m=\u001b[39mlr_schedule(\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs,\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n",
      "File \u001b[0;32m~/miniconda3/envs/tue.cm210908/lib/python3.11/site-packages/stable_baselines3/sac/policies.py:343\u001b[0m, in \u001b[0;36mSACPolicy.make_actor\u001b[0;34m(self, features_extractor)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features_extractor: Optional[BaseFeaturesExtractor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Actor:\n\u001b[1;32m    342\u001b[0m     actor_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_features_extractor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_kwargs, features_extractor)\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mActor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mactor_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/tue.cm210908/lib/python3.11/site-packages/stable_baselines3/sac/policies.py:85\u001b[0m, in \u001b[0;36mActor.__init__\u001b[0;34m(self, observation_space, action_space, net_arch, features_extractor, features_dim, activation_fn, use_sde, log_std_init, full_std, use_expln, clip_mean, normalize_images)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_mean \u001b[38;5;241m=\u001b[39m clip_mean\n\u001b[1;32m     84\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m get_action_dim(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[0;32m---> 85\u001b[0m latent_pi_net \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_arch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_pi \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39mlatent_pi_net)\n\u001b[1;32m     87\u001b[0m last_layer_dim \u001b[38;5;241m=\u001b[39m net_arch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(net_arch) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m features_dim\n",
      "File \u001b[0;32m~/miniconda3/envs/tue.cm210908/lib/python3.11/site-packages/stable_baselines3/common/torch_layers.py:135\u001b[0m, in \u001b[0;36mcreate_mlp\u001b[0;34m(input_dim, output_dim, net_arch, activation_fn, squash_output, with_bias)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03mCreate a multi layer perceptron (MLP), which is\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03ma collection of fully-connected layers each followed by an activation function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(net_arch) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m     modules \u001b[38;5;241m=\u001b[39m [\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_arch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_bias\u001b[49m\u001b[43m)\u001b[49m, activation_fn()]\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     modules \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/tue.cm210908/lib/python3.11/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "\n",
    "seed_set = [1]\n",
    "total_num_episodes = 5000  # Total number of episodes\n",
    "evaluation_interval = 1000\n",
    "learning_rate = 3e-4\n",
    "for seed in seed_set:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Initialize the PPO agent\n",
    "    model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, seed=seed, learning_rate=learning_rate)\n",
    "    # model = TRPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, seed=seed)\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(0, total_num_episodes + 1, evaluation_interval):\n",
    "        if episode > 0:\n",
    "            # Continue training the agent\n",
    "            model.learn(total_timesteps=evaluation_interval)\n",
    "\n",
    "        # Evaluate the agent\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        print(f\"Seed: {seed}, Episode: {episode}, Mean Reward: {mean_reward}, Std Reward: {std_reward}\")\n",
    "        rewards.append((episode, mean_reward))\n",
    "\n",
    "        # Save the model\n",
    "        model.save(f\"{model_path}/{env_name}_ppo_seed_{seed}_episode_{episode}\")\n",
    "\n",
    "    rewards_over_seeds.append(rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting\n",
    "rewards_to_plot = []\n",
    "for seed_index, seed_rewards in enumerate(rewards_over_seeds):\n",
    "    for episode, reward in seed_rewards:\n",
    "        rewards_to_plot.append([seed_set[seed_index], episode, reward])\n",
    " \n",
    "df1 = pd.DataFrame(rewards_to_plot, columns=[\"seed\", \"episodes\", \"reward\"])\n",
    " \n",
    "# Visualize the rewards\n",
    "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\")\n",
    "sns.lineplot(x=\"episodes\", y=\"reward\", hue=\"seed\", data=df1).set(\n",
    "    title=\"PPO for InvertedPendulum-v4\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# Function to visualize the trained model\n",
    "def visualize_trained_model(agent, env_name=\"InvertedPendulum-v4\", num_episodes=1, seed=1):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    frames = []\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset(seed=seed)\n",
    "        done = False\n",
    "        while not done:\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            action = agent.sample_action(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    # Create animation\n",
    "    fig = plt.figure()\n",
    "    plt.axis('off')\n",
    "    im = plt.imshow(frames[0])\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_array(frame)\n",
    "        return [im]\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=frames, interval=50)\n",
    "    plt.close()\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "    frames = [Image.fromarray(frame) for frame in frames]\n",
    "    frames[0].save(f'{demo_path}\\{env_name}_reinforce_seed_{seed}.gif', save_all=True, append_images=frames[1:], loop=0, duration=50)\n",
    "    \n",
    "    return ani\n",
    "\n",
    "visualize_trained_model(model, env_name=\"InvertedPendulum-v4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tue.cm210908",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
