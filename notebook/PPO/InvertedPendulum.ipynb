{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khởi tạo môi trường\n",
    "\n",
    "env_name = \"InvertedPendulum-v4\"\n",
    "# Create and wrap the environment\n",
    "env = gym.make(env_name)\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Records episode-reward\n",
    "\n",
    "# Observation-space of InvertedPendulum-v4 (4)\n",
    "obs_space_dims = env.observation_space.shape[0]\n",
    "# Action-space of InvertedPendulum-v4 (1)\n",
    "action_space_dims = env.action_space.shape[0]\n",
    "rewards_over_seeds = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo folder kết quả\n",
    "\n",
    "import os\n",
    "def mkdir(path): \n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "save_path = \"./save\"\n",
    "env_path = f'{save_path}/{env_name}'\n",
    "model_path = f'{save_path}/{env_name}/model'\n",
    "demo_path = f'{save_path}/{env_name}/demo'\n",
    "\n",
    "mkdir(save_path)\n",
    "mkdir(env_path)\n",
    "mkdir(model_path)\n",
    "mkdir(demo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khai báo thuật toán\n",
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=torch.nn.Tanh,\n",
    "    net_arch=[dict(pi=[128, 128], vf=[128, 128])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "\n",
    "seed_set = [1]\n",
    "total_num_episodes = 5000  # Total number of episodes\n",
    "evaluation_interval = 1000\n",
    "learning_rate = 3e-4\n",
    "for seed in seed_set:\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Initialize the PPO agent\n",
    "    model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, seed=seed, learning_rate=learning_rate)\n",
    "    # model = TRPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, seed=seed)\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(0, total_num_episodes + 1, evaluation_interval):\n",
    "        if episode > 0:\n",
    "            # Continue training the agent\n",
    "            model.learn(total_timesteps=evaluation_interval)\n",
    "\n",
    "        # Evaluate the agent\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        print(f\"Seed: {seed}, Episode: {episode}, Mean Reward: {mean_reward}, Std Reward: {std_reward}\")\n",
    "        rewards.append((episode, mean_reward))\n",
    "\n",
    "        # Save the model\n",
    "        model.save(f\"{model_path}/{env_name}_ppo_seed_{seed}_episode_{episode}\")\n",
    "\n",
    "    rewards_over_seeds.append(rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot kết quả\n",
    "\n",
    "rewards_to_plot = [[reward[0] for reward in rewards] for rewards in rewards_over_seeds]\n",
    "df1 = pd.DataFrame(rewards_to_plot).melt()\n",
    "df1.rename(columns={\"variable\": \"episodes\", \"value\": \"reward\"}, inplace=True)\n",
    "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\")\n",
    "sns.lineplot(x=\"episodes\", y=\"reward\", data=df1).set(\n",
    "    title=\"REINFORCE for InvertedPendulum-v4\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# Function to visualize the trained model\n",
    "def visualize_trained_model(agent, env_name=\"InvertedPendulum-v4\", num_episodes=1, seed=1):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    frames = []\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset(seed=seed)\n",
    "        done = False\n",
    "        while not done:\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            action = agent.sample_action(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    # Create animation\n",
    "    fig = plt.figure()\n",
    "    plt.axis('off')\n",
    "    im = plt.imshow(frames[0])\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_array(frame)\n",
    "        return [im]\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=frames, interval=50)\n",
    "    plt.close()\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "    frames = [Image.fromarray(frame) for frame in frames]\n",
    "    frames[0].save(f'{demo_path}\\{env_name}_reinforce_seed_{seed}.gif', save_all=True, append_images=frames[1:], loop=0, duration=50)\n",
    "    \n",
    "    return ani\n",
    "\n",
    "# Assuming the REINFORCE class and agent are defined and trained as in your provided code\n",
    "visualize_trained_model(agent, env_name=\"InvertedPendulum-v4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tue.cm210908",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
